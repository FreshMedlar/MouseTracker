{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import data_loader\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50] Batch 0/670                       Loss D: 0.4178, loss G: 0.2444\n",
      "Epoch [1/50] Batch 0/670                       Loss D: 0.0748, loss G: 0.3806\n",
      "Epoch [2/50] Batch 0/670                       Loss D: 0.0260, loss G: 0.6049\n",
      "Epoch [3/50] Batch 0/670                       Loss D: 0.0075, loss G: 0.7837\n",
      "Epoch [4/50] Batch 0/670                       Loss D: 0.0030, loss G: 0.8644\n",
      "Epoch [5/50] Batch 0/670                       Loss D: 0.0010, loss G: 0.9165\n",
      "Epoch [6/50] Batch 0/670                       Loss D: 0.0006, loss G: 0.9380\n",
      "Epoch [7/50] Batch 0/670                       Loss D: 0.0004, loss G: 0.9436\n",
      "Epoch [8/50] Batch 0/670                       Loss D: 0.0002, loss G: 0.9639\n",
      "Epoch [9/50] Batch 0/670                       Loss D: 0.0001, loss G: 0.9674\n",
      "Epoch [10/50] Batch 0/670                       Loss D: 0.0001, loss G: 0.9748\n",
      "Epoch [11/50] Batch 0/670                       Loss D: 0.0001, loss G: 0.9807\n",
      "Epoch [12/50] Batch 0/670                       Loss D: 0.0000, loss G: 0.9816\n",
      "Epoch [13/50] Batch 0/670                       Loss D: 0.0000, loss G: 0.9872\n",
      "Epoch [14/50] Batch 0/670                       Loss D: 0.0000, loss G: 0.9891\n",
      "Epoch [15/50] Batch 0/670                       Loss D: 0.0000, loss G: 0.9906\n",
      "Epoch [16/50] Batch 0/670                       Loss D: 0.0031, loss G: 0.9463\n",
      "Epoch [17/50] Batch 0/670                       Loss D: 0.0011, loss G: 0.9837\n",
      "Epoch [18/50] Batch 0/670                       Loss D: 0.0006, loss G: 0.9594\n",
      "Epoch [19/50] Batch 0/670                       Loss D: 0.0000, loss G: 0.9988\n",
      "Epoch [20/50] Batch 0/670                       Loss D: 0.0000, loss G: 0.9999\n",
      "Epoch [21/50] Batch 0/670                       Loss D: 0.0000, loss G: 0.9997\n",
      "Epoch [22/50] Batch 0/670                       Loss D: 0.0000, loss G: 0.9999\n",
      "Epoch [23/50] Batch 0/670                       Loss D: 0.0000, loss G: 0.9995\n",
      "Epoch [24/50] Batch 0/670                       Loss D: 0.0001, loss G: 0.9923\n",
      "Epoch [25/50] Batch 0/670                       Loss D: 0.0000, loss G: 0.9999\n",
      "Epoch [26/50] Batch 0/670                       Loss D: 0.0000, loss G: 1.0000\n",
      "Epoch [27/50] Batch 0/670                       Loss D: 0.0000, loss G: 1.0000\n",
      "Epoch [28/50] Batch 0/670                       Loss D: 0.0000, loss G: 1.0000\n",
      "Epoch [29/50] Batch 0/670                       Loss D: 0.0000, loss G: 1.0000\n",
      "Epoch [30/50] Batch 0/670                       Loss D: 0.0000, loss G: 1.0000\n",
      "Epoch [31/50] Batch 0/670                       Loss D: 0.0000, loss G: 1.0000\n",
      "Epoch [32/50] Batch 0/670                       Loss D: 0.0000, loss G: 1.0000\n",
      "Epoch [33/50] Batch 0/670                       Loss D: 0.0000, loss G: 1.0000\n",
      "Epoch [34/50] Batch 0/670                       Loss D: 0.0000, loss G: 1.0000\n",
      "Epoch [35/50] Batch 0/670                       Loss D: 0.0000, loss G: 1.0000\n",
      "Epoch [36/50] Batch 0/670                       Loss D: 0.0000, loss G: 1.0000\n",
      "Epoch [37/50] Batch 0/670                       Loss D: 0.0000, loss G: 1.0000\n",
      "Epoch [38/50] Batch 0/670                       Loss D: 0.0000, loss G: 1.0000\n",
      "Epoch [39/50] Batch 0/670                       Loss D: 0.0000, loss G: 1.0000\n",
      "Epoch [40/50] Batch 0/670                       Loss D: 0.0000, loss G: 0.9988\n",
      "Epoch [41/50] Batch 0/670                       Loss D: 0.0000, loss G: 1.0000\n",
      "Epoch [42/50] Batch 0/670                       Loss D: 0.0000, loss G: 0.9996\n",
      "Epoch [43/50] Batch 0/670                       Loss D: 0.0000, loss G: 1.0000\n",
      "Epoch [44/50] Batch 0/670                       Loss D: 0.0000, loss G: 1.0000\n",
      "Epoch [45/50] Batch 0/670                       Loss D: 0.0000, loss G: 1.0000\n",
      "Epoch [46/50] Batch 0/670                       Loss D: 0.0000, loss G: 1.0000\n",
      "Epoch [47/50] Batch 0/670                       Loss D: 0.0000, loss G: 1.0000\n",
      "Epoch [48/50] Batch 0/670                       Loss D: 0.0000, loss G: 1.0000\n",
      "Epoch [49/50] Batch 0/670                       Loss D: 0.0000, loss G: 1.0000\n"
     ]
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(in_features, 25).to(torch.float64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(25, 1).to(torch.float64),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, out_features): # z_dim is the dimension of the noise (latent_dimension)\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(z_dim, 15).to(torch.float64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(15, out_features).to(torch.float64),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)\n",
    "    \n",
    "device = \"cpu\"\n",
    "lr = 3e-4\n",
    "z_dim = 6\n",
    "seq_dimension = 300\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "\n",
    "disc = Discriminator(seq_dimension).to(device)\n",
    "gen = Generator(z_dim, seq_dimension).to(device)\n",
    "\n",
    "fixed_noise = torch.randn(batch_size, z_dim, dtype=torch.float64).to(device)\n",
    "\n",
    "destination, dataset, time = data_loader.load_data(\"data.json\")\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()\n",
    "writer_fake = SummaryWriter(f\"runs/GAN_seq/fake\")\n",
    "writer_real = SummaryWriter(f\"runs/GAN_seq/real\")\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # for batch_idx, (real, _) in enumerate(loader):\n",
    "    for batch_idx, real in enumerate(loader):\n",
    "        # real = real.view(-1, 784).to(device)\n",
    "        real = real.view(-1, 300)\n",
    "        batch_size = real.shape[0]\n",
    "\n",
    "        ### Train discriminator: max log(D(real)) + log(1 - D(G(z)))\n",
    "        noise = torch.randn(batch_size, z_dim, dtype=torch.float64).to(device)\n",
    "        fake = gen(noise)\n",
    "        disc_real = disc(real).view(-1)\n",
    "        lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "\n",
    "        disc_fake = disc(fake).view(-1)\n",
    "        lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        lossD = (lossD_real + lossD_fake)/2\n",
    "        disc.zero_grad()\n",
    "        lossD.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "\n",
    "        ### Train the Generator min log(1 - D(G(z))) <-> max log(D(G(z)))\n",
    "        output = disc(fake).view(-1)\n",
    "        lossG = criterion(output, torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "                      Loss D: {lossD:.4f}, loss G: {lossG:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise).reshape(-1, 1, 100, 3)\n",
    "                data = real.reshape(-1, 1, 100, 3)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
    "                img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
    "\n",
    "                writer_fake.add_image(\n",
    "                    \"Mnist Fake Images\", img_grid_fake, global_step=step\n",
    "                )\n",
    "                writer_real.add_image(\n",
    "                    \"Mnist Real Images\", img_grid_real, global_step=step\n",
    "                )\n",
    "                step += 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "300\n",
      "tensor([ 0.0000e+00,  1.0000e+00,  0.0000e+00,  9.7278e-04,  2.0000e+00,\n",
      "         0.0000e+00,  1.9455e-03,  7.0000e+00,  0.0000e+00,  2.9183e-03,\n",
      "         8.0000e+00,  0.0000e+00,  3.8911e-03,  9.0000e+00,  0.0000e+00,\n",
      "         4.8638e-03,  1.2000e+01, -5.2083e-04,  6.8094e-03,  1.4000e+01,\n",
      "        -5.2083e-04,  7.7821e-03,  1.7000e+01, -5.2083e-04,  8.7549e-03,\n",
      "         1.8000e+01, -5.2083e-04,  9.7276e-03,  1.8000e+01, -5.2083e-04,\n",
      "         1.0700e-02,  2.2000e+01, -5.2083e-04,  1.1673e-02,  2.3000e+01,\n",
      "        -5.2083e-04,  1.2646e-02,  2.5000e+01, -5.2083e-04,  1.3619e-02,\n",
      "         2.6000e+01, -5.2083e-04,  1.4591e-02,  2.6000e+01, -5.2083e-04,\n",
      "         1.5564e-02,  3.1000e+01, -5.2083e-04,  1.6537e-02,  3.2000e+01,\n",
      "        -5.2083e-04,  1.7510e-02,  3.3000e+01, -5.2083e-04,  1.8483e-02,\n",
      "         3.6000e+01, -5.2083e-04,  1.9455e-02,  3.7000e+01, -5.2083e-04,\n",
      "         2.0428e-02,  4.2000e+01, -1.5625e-03,  2.0428e-02,  4.3000e+01,\n",
      "        -2.0833e-03,  2.1401e-02,  4.4000e+01, -2.0833e-03,  2.2374e-02,\n",
      "         4.7000e+01, -2.0833e-03,  2.2374e-02,  5.0000e+01, -2.6042e-03,\n",
      "         2.3346e-02,  5.6000e+01, -2.6042e-03,  2.4319e-02,  5.9000e+01,\n",
      "        -2.6042e-03,  2.5292e-02,  1.2000e+02, -2.6042e-03,  2.6265e-02,\n",
      "         1.3400e+02, -2.6042e-03,  2.7237e-02,  1.3500e+02, -2.6042e-03,\n",
      "         2.8210e-02,  1.4200e+02, -2.6042e-03,  2.9183e-02,  1.4800e+02,\n",
      "        -1.5625e-03,  3.0156e-02,  1.5200e+02, -1.5625e-03,  3.1128e-02,\n",
      "         1.5300e+02, -1.0417e-03,  3.3074e-02,  1.5600e+02,  5.2083e-04,\n",
      "         3.3074e-02,  1.5800e+02,  1.0417e-03,  3.6965e-02,  1.6100e+02,\n",
      "         2.0833e-03,  3.6965e-02,  1.6200e+02,  2.6042e-03,  3.6965e-02,\n",
      "         1.6400e+02,  3.1250e-03,  3.8911e-02,  1.6600e+02,  4.1667e-03,\n",
      "         3.9883e-02,  1.6700e+02,  4.6875e-03,  3.9883e-02,  1.6900e+02,\n",
      "         5.2083e-03,  3.9883e-02,  1.7000e+02,  5.7292e-03,  4.1829e-02,\n",
      "         1.7100e+02,  6.7708e-03,  4.2802e-02,  1.7200e+02,  6.7708e-03,\n",
      "         4.3774e-02,  1.7400e+02,  7.8125e-03,  4.3774e-02,  1.7500e+02,\n",
      "         8.3333e-03,  4.3774e-02,  1.7700e+02,  8.8542e-03,  4.4747e-02,\n",
      "         1.7800e+02,  9.3750e-03,  4.4747e-02,  1.8000e+02,  1.0417e-02,\n",
      "         4.6693e-02,  1.8100e+02,  1.0937e-02,  4.6693e-02,  1.8200e+02,\n",
      "         1.1458e-02,  4.7665e-02,  1.8300e+02,  1.1979e-02,  4.7665e-02,\n",
      "         1.8400e+02,  1.2500e-02,  4.7665e-02,  1.8500e+02,  1.3021e-02,\n",
      "         4.8638e-02,  1.8700e+02,  1.3542e-02,  4.9611e-02,  1.8800e+02,\n",
      "         1.4063e-02,  4.9611e-02,  1.8900e+02,  1.4583e-02,  4.9611e-02,\n",
      "         1.9100e+02,  1.5104e-02,  5.0584e-02,  1.9200e+02,  1.6146e-02,\n",
      "         5.0584e-02,  1.9500e+02,  1.6667e-02,  5.1556e-02,  1.9600e+02,\n",
      "         1.6667e-02,  5.2529e-02,  1.9700e+02,  1.7187e-02,  5.2529e-02,\n",
      "         2.0000e+02,  1.7708e-02,  5.2529e-02,  2.0200e+02,  1.8229e-02,\n",
      "         5.3502e-02,  2.0800e+02,  1.9271e-02,  5.3502e-02,  2.1000e+02,\n",
      "         1.9792e-02,  5.3502e-02,  2.1800e+02,  2.0312e-02,  5.3502e-02,\n",
      "         2.1900e+02,  2.0833e-02,  5.3502e-02,  2.2500e+02,  2.1354e-02,\n",
      "         5.5447e-02,  2.4700e+02,  2.2396e-02,  5.5447e-02,  2.4800e+02,\n",
      "         2.2917e-02,  5.5447e-02,  2.6300e+02,  2.3438e-02,  5.5447e-02,\n",
      "         2.6900e+02,  2.3958e-02,  5.5447e-02,  2.7200e+02,  2.4479e-02,\n",
      "         5.6420e-02,  2.9100e+02,  2.5000e-02,  5.6420e-02,  2.9800e+02,\n",
      "         2.5521e-02,  5.7393e-02,  2.9900e+02,  2.6042e-02,  5.7393e-02,\n",
      "         3.0900e+02,  2.6563e-02,  5.7393e-02,  3.1200e+02,  2.7083e-02,\n",
      "         5.7393e-02,  3.1300e+02,  2.7604e-02,  5.6420e-02,  3.1600e+02,\n",
      "         2.7604e-02,  5.6420e-02,  3.1700e+02,  2.8125e-02,  5.5447e-02,\n",
      "         3.1800e+02,  2.8125e-02,  5.4475e-02,  3.2000e+02,  2.8125e-02,\n",
      "         5.4475e-02,  3.2100e+02,  2.9167e-02,  5.2529e-02,  3.2200e+02,\n",
      "         2.9688e-02,  5.0584e-02,  3.2500e+02,  3.1250e-02,  4.7665e-02,\n",
      "         3.2600e+02,  3.1771e-02,  4.7665e-02,  3.2700e+02,  3.3333e-02,\n",
      "         4.4747e-02,  3.2800e+02,  3.3854e-02,  4.0856e-02,  3.3100e+02,\n",
      "         3.5417e-02,  4.0856e-02,  3.3200e+02,  3.7500e-02,  3.7938e-02,\n",
      "         3.3300e+02,  3.7500e-02,  3.6965e-02,  3.3400e+02,  3.9583e-02,\n",
      "         3.5992e-02,  3.3500e+02,  4.0104e-02,  3.5992e-02,  3.3600e+02,\n",
      "         4.1667e-02,  3.4047e-02,  3.3700e+02,  4.3750e-02,  2.8210e-02,\n",
      "         3.3900e+02,  4.6354e-02,  2.4319e-02,  3.4100e+02,  5.1042e-02],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "destination, path, time = data_loader.load_data(\"data.json\")\n",
    "# print(len(destination))\n",
    "# print(len(path[0]))\n",
    "# print(len(time))\n",
    "batch_size = 32\n",
    "\n",
    "loader = DataLoader(path, batch_size=batch_size, shuffle=True)\n",
    "for index, real in enumerate(loader):\n",
    "    real = real.view(-1, 300)\n",
    "    if index == 0:\n",
    "        print(real.shape[0])\n",
    "        print(len(real[0]))\n",
    "        print(real[0])\n",
    "# for i in range(len(path)):\n",
    "#     if i == 0:\n",
    "#         print(path[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7320,  1.3747, -0.3393],\n",
      "        [-0.4281,  0.0908, -0.1422],\n",
      "        [-0.9475,  0.2882, -0.9761],\n",
      "        [ 0.2710,  1.5331, -1.3416],\n",
      "        [ 0.1415, -1.1231,  0.8345],\n",
      "        [ 0.7772, -0.1877, -0.7764],\n",
      "        [-0.2638, -0.4558,  0.0333],\n",
      "        [ 0.1946, -0.0061,  1.1337],\n",
      "        [-0.9534,  1.3972, -1.8519],\n",
      "        [ 0.5256,  0.3138, -1.4106]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fixed_noise = torch.randn((10, 3))\n",
    "print(fixed_noise)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
